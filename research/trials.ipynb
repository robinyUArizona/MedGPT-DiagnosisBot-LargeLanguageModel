{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# import logging\n",
    "# logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "     # Read the text content from the file\n",
    "     loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "     documents = loader.load()\n",
    "     return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Move one level up and then into the 'data' folder\n",
    "data_folder_path = os.path.join(current_dir, '..', 'data')\n",
    "extracted_data = load_pdf_file(data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Text Chunks\n",
    "# Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
    "# Balances between maintaining coherence and adhering to character limits.\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 7035\n",
      "Sample chunk:\n",
      "TheGALE\n",
      "ENCYCLOPEDIA\n",
      "ofMEDICINE\n",
      "SECOND EDITION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(f\"Number of document chunks: {len(text_chunks)}\")\n",
    "print(f\"Sample chunk:\\n{text_chunks[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #`384` dimensional \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "# check embedding works or not\n",
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'medicalchatbot' has been created.\n"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Initialize Pinecone gRPC client instance\n",
    "pc = Pinecone()\n",
    "\n",
    "# Define the index name for the medical chatbot embeddings\n",
    "index_name = \"medicalchatbot\"\n",
    "\n",
    "pc.create_index(\n",
    "    name = index_name,            # Name of the index to be created\n",
    "    dimension = 384,              # Dimension size of the embeddings (replace with your model's embedding size)\n",
    "    metric = \"cosine\",            # Distance metric for nearest neighbor search (e.g., cosine similarity)\n",
    "    spec = ServerlessSpec(        # Serverless configuration\n",
    "        cloud=\"aws\",            # Cloud provider where the index is hosted\n",
    "        region=\"us-east-1\"      # Specific region to deploy the index\n",
    "    )\n",
    ")\n",
    "print(f\"Index '{index_name}' has been created.\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents = text_chunks,\n",
    "    embedding = embeddings, \n",
    "    index_name = index_name  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index from Pinecone (vector store) index \n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name = index_name,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1ed23e90eb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's question\n",
    "query = \"What is Acne?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = docsearch.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "Acidosis seeRespiratory acidosis; Renal\n",
      "tubular acidosis; Metabolic acidosis\n",
      "Acne\n",
      "Definition\n",
      "Acne is a common skin disease characterized by\n",
      "pimples on the face, chest, and back. It occurs when thepores of the skin become clogged with oil, dead skincells, and bacteria.\n",
      "Description\n",
      "Acne vulgaris, the medical term for common acne, is\n",
      "\n",
      "Source: c:\\Users\\yrobi\\Desktop\\Robin World\\Data Science - Machine Learning Prep\\01 - Generative AI\\MedGPT-DiagnosisBot-LargeLanguageModel\\research\\..\\data\\Medical_book.pdf\n",
      "\n",
      "Document 2:\n",
      "GALE ENCYCLOPEDIA OF MEDICINE 2 25Acne\n",
      "Acne vulgaris affecting a womanâ€™s face. Acne is the general\n",
      "name given to a skin disorder in which the sebaceousglands become inflamed. (Photograph by Biophoto Associ-\n",
      "ates, Photo Researchers, Inc. Reproduced by permission.)GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25\n",
      "\n",
      "Source: c:\\Users\\yrobi\\Desktop\\Robin World\\Data Science - Machine Learning Prep\\01 - Generative AI\\MedGPT-DiagnosisBot-LargeLanguageModel\\research\\..\\data\\Medical_book.pdf\n",
      "\n",
      "Document 3:\n",
      "ent purposes. For example, lotions, soaps, gels, andcreams containing benzoyl peroxide or tretinoin may beused to clear up mild to moderately severe acne.Isotretinoin (Accutane) is prescribed only for verysevere, disfiguring acne.\n",
      "Acne is a skin condition that occurs when pores or\n",
      "\n",
      "Source: c:\\Users\\yrobi\\Desktop\\Robin World\\Data Science - Machine Learning Prep\\01 - Generative AI\\MedGPT-DiagnosisBot-LargeLanguageModel\\research\\..\\data\\Medical_book.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatOpenAI model\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Prompt with System and Human Messages (Using Tuples)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Acromegaly and gigantism are medical conditions that involve excessive growth hormone production, leading to abnormal growth of the bones and tissues. They are characterized by enlarged body parts, such as the hands, feet, and face. Treatment for these conditions may include medication, surgery, or radiation therapy.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stats is a shortened term for statistics, which is the practice of collecting, analyzing, and interpreting data. It involves using mathematical and analytical techniques to understand and make conclusions about a particular set of data. It is commonly used in fields such as data science, economics, and social sciences.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical chatbot RAG conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt  = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for contextualizing questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate a continual chat\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        # Update the chat history\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n",
      "AI: Acne is a common skin disease characterized by pimples on the face, chest, and back, resulting from clogged pores due to oil, dead skin cells, and bacteria. The medical term for common acne is acne vulgaris, which involves inflammation of the sebaceous glands. Treatment options include topical lotions, soaps, gels, and, for severe cases, isotretinoin (Accutane).\n",
      "AI: Robins Yadav is a data professional with a master's degree specializing in Data Science and Machine Learning. He is proficient in Python, SQL, AWS, and Big Data, and has expertise in healthcare analytics, NLP, and LLM. Robins also has experience in model development and MLOps for deploying scalable machine learning solutions.\n"
     ]
    }
   ],
   "source": [
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
